{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f09c1205",
   "metadata": {},
   "source": [
    "Instalación de Librerías\n",
    "Para ejecutar este modelo, necesitas las siguientes bibliotecas. Instálalas con el siguiente comando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65805f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\andfe\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: transformers in c:\\users\\andfe\\anaconda3\\lib\\site-packages (4.41.1)\n",
      "Requirement already satisfied: torch in c:\\users\\andfe\\anaconda3\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\andfe\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\andfe\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\andfe\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\andfe\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\andfe\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\andfe\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in c:\\users\\andfe\\anaconda3\\lib\\site-packages (from transformers) (0.23.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\andfe\\anaconda3\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\andfe\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\andfe\\anaconda3\\lib\\site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in c:\\users\\andfe\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\andfe\\anaconda3\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\andfe\\anaconda3\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\andfe\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\andfe\\anaconda3\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\andfe\\anaconda3\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\andfe\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\andfe\\anaconda3\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\andfe\\anaconda3\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\andfe\\anaconda3\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\andfe\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\andfe\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\andfe\\anaconda3\\lib\\site-packages (from scikit-learn) (3.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\andfe\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\andfe\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.12.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\andfe\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\andfe\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\andfe\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\andfe\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\andfe\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\andfe\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andfe\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\andfe\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas transformers torch scikit-learn numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5826cde8",
   "metadata": {},
   "source": [
    "Fase 1: Preprocesamiento de Datos en Inglés"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "face1e30",
   "metadata": {},
   "source": [
    "1.1 Cargar datos y mostrar tamaño del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e85cb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de librerías necesarias\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from transformers import AdamW\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8faaaf79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos cargados. Tamaño del dataset: (207433, 16)\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos desde la ubicación especificada\n",
    "data = pd.read_csv(r'C:\\Users\\andfe\\Documents\\Modelo Sentimientos Despliegue\\processed_news_dataset.csv')\n",
    "print(\"Datos cargados. Tamaño del dataset:\", data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6d7a4c",
   "metadata": {},
   "source": [
    "Esta sección carga los datos y muestra el tamaño del dataset para verificar que se haya importado correctamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a405fe",
   "metadata": {},
   "source": [
    "1.2 Tokenización y Preparación para BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ee801c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos después de eliminar valores nulos en 'processed_text' y asegurar que sean cadenas. Tamaño del dataset: (207431, 16)\n",
      "Tamaño del dataset reducido: (20743, 16)\n"
     ]
    }
   ],
   "source": [
    "# Eliminar filas donde `processed_text` o `category` es NaN y asegurar que todos los textos sean cadenas\n",
    "data = data.dropna(subset=['processed_text', 'category'])\n",
    "data = data[data['processed_text'].apply(lambda x: isinstance(x, str))]\n",
    "\n",
    "print(\"Datos después de eliminar valores nulos en 'processed_text' y asegurar que sean cadenas. Tamaño del dataset:\", data.shape)\n",
    "\n",
    "\n",
    "# Reducir el dataset al 10% de su tamaño original\n",
    "sample_fraction = 0.1  \n",
    "data = data.sample(frac=sample_fraction, random_state=42)\n",
    "print(\"Tamaño del dataset reducido:\", data.shape)\n",
    "\n",
    "\n",
    "# Configuración del tokenizador BERT\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Parámetros de tokenización\n",
    "max_length = 128\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# Tokenización de los textos\n",
    "for text in data['processed_text']:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    # Añadir los tensores resultantes como elementos individuales en las listas\n",
    "    input_ids.append(encoded_dict['input_ids'].squeeze(0))  \n",
    "    attention_masks.append(encoded_dict['attention_mask'].squeeze(0))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb53ff36",
   "metadata": {},
   "source": [
    "Aquí, utilizamos el tokenizador de BERT para transformar los textos en una representación compatible. Se ajustan las secuencias a una longitud máxima y se añade relleno para asegurar uniformidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e329d3b",
   "metadata": {},
   "source": [
    "1.3 Codificación de Etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0854dea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Codificación de etiquetas completada. Clases encontradas: ['ARTS & CULTURE' 'BUSINESS' 'COMEDY' 'CRIME' 'CULTURE & ARTS'\n",
      " 'DIVERSITY VOICES' 'DIVORCE' 'EDUCATION' 'ENTERTAINMENT' 'ENVIRONMENT'\n",
      " 'FOOD & DRINK' 'HOME & LIVING' 'IMPACT' 'MEDIA' 'PARENTS' 'POLITICS'\n",
      " 'RELIGION' 'SCIENCE' 'SPORTS' 'STYLE & BEAUTY' 'TECH' 'TRAVEL' 'VARIETY'\n",
      " 'WEDDINGS' 'WELLNESS' 'WOMEN' 'WORLD NEWS']\n"
     ]
    }
   ],
   "source": [
    "# Codificación de etiquetas\n",
    "label_encoder = LabelEncoder()\n",
    "labels = torch.tensor(label_encoder.fit_transform(data['category']))\n",
    "\n",
    "print(\"Codificación de etiquetas completada. Clases encontradas:\", label_encoder.classes_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e185f0",
   "metadata": {},
   "source": [
    "Convertimos las categorías de noticias en etiquetas numéricas, lo que permite que el modelo las entienda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea5c4c9",
   "metadata": {},
   "source": [
    "1.4 Conversión a Tensores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98d00763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño de input_ids: torch.Size([20743, 128])\n",
      "Tamaño de attention_masks: torch.Size([20743, 128])\n",
      "Tamaño de labels: torch.Size([20743])\n",
      "Conversión a tensores completada. Dataset preparado.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "import torch\n",
    "\n",
    "# Verificar que todos los elementos sean tensores\n",
    "input_ids = [torch.tensor(tensor) if not isinstance(tensor, torch.Tensor) else tensor for tensor in input_ids]\n",
    "attention_masks = [torch.tensor(tensor) if not isinstance(tensor, torch.Tensor) else tensor for tensor in attention_masks]\n",
    "\n",
    "# Convertir las listas de tensores en un solo tensor apilado\n",
    "input_ids = torch.stack(input_ids, dim=0)\n",
    "attention_masks = torch.stack(attention_masks, dim=0)\n",
    "\n",
    "\n",
    "# Codificación de etiquetas\n",
    "label_encoder = LabelEncoder()\n",
    "labels = torch.tensor(label_encoder.fit_transform(data['category']), dtype=torch.long)\n",
    "\n",
    "\n",
    "# Verificar que los tamaños coinciden\n",
    "print(\"Tamaño de input_ids:\", input_ids.size())\n",
    "print(\"Tamaño de attention_masks:\", attention_masks.size())\n",
    "print(\"Tamaño de labels:\", labels.size())\n",
    "\n",
    "# Creación del dataset\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "print(\"Conversión a tensores completada. Dataset preparado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c707d2",
   "metadata": {},
   "source": [
    "Agrupamos los datos en un TensorDataset, una estructura que PyTorch requiere para manejar datos en lotes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd181d14",
   "metadata": {},
   "source": [
    "Fase 2: Configuración del Modelo BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd2f00f",
   "metadata": {},
   "source": [
    "2.1 Carga del Modelo de Clasificación BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "605cc7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo BERT para clasificación cargado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label_encoder.classes_),\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "print(\"Modelo BERT para clasificación cargado exitosamente.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4cad3d",
   "metadata": {},
   "source": [
    "Aquí se carga el modelo BERT preentrenado y se adapta para la clasificación de múltiples categorías, con el número de etiquetas igual a las categorías de noticias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee12ce0",
   "metadata": {},
   "source": [
    "2.2 Configuración de DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7491abfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders configurados. Tamaño del conjunto de entrenamiento: 16594, tamaño de validación: 4149\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "print(f\"DataLoaders configurados. Tamaño del conjunto de entrenamiento: {train_size}, tamaño de validación: {val_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7230093",
   "metadata": {},
   "source": [
    "Aquí dividimos los datos en conjuntos de entrenamiento y validación, y configuramos los DataLoaders para manejar el procesamiento en lotes durante el entrenamiento y validación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83750cc6",
   "metadata": {},
   "source": [
    "Fase 3: Entrenamiento del Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9a0d33",
   "metadata": {},
   "source": [
    "3.1 Configuración del Optimizador AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d533aa9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andfe\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizador AdamW configurado con tasa de aprendizaje: 2e-05\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "print(\"Optimizador AdamW configurado con tasa de aprendizaje:\", 2e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514dedcd",
   "metadata": {},
   "source": [
    "Definimos el optimizador AdamW, que es especialmente adecuado para ajustar modelos BERT, con una tasa de aprendizaje recomendada para mantener la estabilidad del entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4ee39c",
   "metadata": {},
   "source": [
    "3.2 Entrenamiento del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1aa4c4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Pérdida Promedio de Entrenamiento: 1.8881\n",
      "Precisión Promedio en Validación para Epoch 1: 0.5011\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "max_steps_per_epoch = 20  \n",
    "\n",
    "# Entrenamiento del modelo\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step >= max_steps_per_epoch:\n",
    "            break\n",
    "        \n",
    "        b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "        model.zero_grad()\n",
    "        \n",
    "        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "    \n",
    "    avg_train_loss = total_loss / (step + 1)  \n",
    "    print(f\"Epoch {epoch+1} - Pérdida Promedio de Entrenamiento: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    # Validación después de cada época\n",
    "    model.eval()\n",
    "    val_accuracy = 0\n",
    "    for batch in val_dataloader:\n",
    "        b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1).flatten()\n",
    "        val_accuracy += (predictions == b_labels).cpu().numpy().mean()\n",
    "    \n",
    "    avg_val_accuracy = val_accuracy / len(val_dataloader)\n",
    "    print(f\"Precisión Promedio en Validación para Epoch {epoch+1}: {avg_val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8e4bdd",
   "metadata": {},
   "source": [
    "El modelo se entrena por varias épocas. En cada época, se calcula y muestra la pérdida de entrenamiento promedio. Después de cada época, se evalúa el modelo en el conjunto de validación, mostrando la precisión promedio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcc4542",
   "metadata": {},
   "source": [
    "Fase 4: Evaluación del Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b049b505",
   "metadata": {},
   "source": [
    "4.1 Evaluación en el Conjunto de Validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acf441e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporte de Clasificación:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "  ARTS & CULTURE       0.00      0.00      0.00        32\n",
      "        BUSINESS       0.54      0.33      0.41       159\n",
      "          COMEDY       0.00      0.00      0.00        99\n",
      "           CRIME       0.67      0.03      0.06        66\n",
      "  CULTURE & ARTS       0.00      0.00      0.00        57\n",
      "DIVERSITY VOICES       0.42      0.08      0.13       222\n",
      "         DIVORCE       0.87      0.16      0.27        82\n",
      "       EDUCATION       0.00      0.00      0.00        44\n",
      "   ENTERTAINMENT       0.40      0.80      0.53       351\n",
      "     ENVIRONMENT       0.00      0.00      0.00        69\n",
      "    FOOD & DRINK       0.54      0.83      0.66       155\n",
      "   HOME & LIVING       0.69      0.10      0.18       106\n",
      "          IMPACT       0.00      0.00      0.00        62\n",
      "           MEDIA       0.00      0.00      0.00        63\n",
      "         PARENTS       0.57      0.55      0.56       277\n",
      "        POLITICS       0.60      0.84      0.70       702\n",
      "        RELIGION       0.00      0.00      0.00        40\n",
      "         SCIENCE       1.00      0.02      0.04        51\n",
      "          SPORTS       0.75      0.59      0.66       100\n",
      "  STYLE & BEAUTY       0.47      0.80      0.59       230\n",
      "            TECH       0.00      0.00      0.00        41\n",
      "          TRAVEL       0.37      0.82      0.51       180\n",
      "         VARIETY       0.33      0.01      0.02       106\n",
      "        WEDDINGS       1.00      0.03      0.05        75\n",
      "        WELLNESS       0.48      0.80      0.60       470\n",
      "           WOMEN       0.00      0.00      0.00        81\n",
      "      WORLD NEWS       0.51      0.27      0.35       229\n",
      "\n",
      "        accuracy                           0.50      4149\n",
      "       macro avg       0.38      0.26      0.23      4149\n",
      "    weighted avg       0.46      0.50      0.42      4149\n",
      "\n",
      "Matriz de Confusión:\n",
      " [[  0   0   0   0   0   0   0   0  15   0   0   0   0   0   2   2   0   0\n",
      "    0   3   0   5   0   0   5   0   0]\n",
      " [  0  53   0   0   0   1   0   0   5   0  14   2   0   0   8  34   0   0\n",
      "    0   5   0  15   0   0  22   0   0]\n",
      " [  0   1   0   0   0   0   0   0  52   0   4   0   0   0   5  15   0   0\n",
      "    2   2   0   6   0   0  12   0   0]\n",
      " [  0   0   0   2   0   2   0   0   7   0   0   0   0   0   5  18   0   0\n",
      "    1   2   0   4   1   0   6   0  18]\n",
      " [  0   0   0   0   0   1   0   0  19   0   1   0   0   0   1   1   0   0\n",
      "    0   9   0  16   0   0   9   0   0]\n",
      " [  0   0   0   0   0  17   0   0  78   0   4   1   0   0  13  42   0   0\n",
      "    3  13   0   7   0   0  36   0   8]\n",
      " [  0   1   0   0   0   0  13   0  29   0   1   0   0   0   4   0   0   0\n",
      "    0   4   0   0   0   0  29   0   1]\n",
      " [  0   3   0   0   0   1   0   0   4   0   1   0   0   0   4  18   0   0\n",
      "    0   0   0   3   0   0  10   0   0]\n",
      " [  0   0   0   0   0   0   0   0 281   0   4   0   0   0   2  14   0   0\n",
      "    3  27   0  10   0   0   8   0   2]\n",
      " [  0   2   0   0   0   0   0   0   1   0   4   1   0   0   3  19   0   0\n",
      "    0   3   0  23   0   0  10   0   3]\n",
      " [  0   1   0   0   0   0   0   0   2   0 129   0   0   0   3   1   0   0\n",
      "    0   4   0   7   0   0   8   0   0]\n",
      " [  0   1   0   0   0   0   0   0   3   0  15  11   0   0   2   0   0   0\n",
      "    0  35   0  32   0   0   7   0   0]\n",
      " [  0   6   0   0   0   0   0   0   4   0   2   0   0   0  10  16   0   0\n",
      "    0   0   0   3   0   0  19   0   2]\n",
      " [  0   4   0   0   0   1   0   0  18   0   0   0   0   0   0  32   0   0\n",
      "    0   3   0   4   0   0   1   0   0]\n",
      " [  0   0   0   0   0   0   0   0  18   0  10   0   0   0 152   5   0   0\n",
      "    3  13   0   9   0   0  67   0   0]\n",
      " [  0   3   0   0   0   5   0   0  47   0   3   1   0   0   9 591   0   0\n",
      "    2   3   0   7   0   0  21   0  10]\n",
      " [  0   0   0   0   0   4   0   0   3   0   1   0   0   0   0  12   0   0\n",
      "    0   0   0   4   0   0  12   0   4]\n",
      " [  0   0   0   0   0   0   0   0   4   0   2   0   0   0   2   2   0   1\n",
      "    0   1   0  13   0   0  22   0   4]\n",
      " [  0   0   0   0   0   0   0   0  18   0   0   0   0   0   2   5   0   0\n",
      "   59   2   0   6   1   0   5   0   2]\n",
      " [  0   0   0   0   0   0   0   0  14   0   5   0   0   0   3   4   0   0\n",
      "    1 185   0   4   0   0  14   0   0]\n",
      " [  0   9   0   0   0   1   0   0   3   0   1   0   0   0   2   9   0   0\n",
      "    0   3   0   5   0   0   7   0   1]\n",
      " [  0   1   0   0   0   0   0   0   2   0  11   0   0   0   1   4   0   0\n",
      "    1   2   0 147   0   0   9   0   2]\n",
      " [  0   2   0   1   0   0   0   0  14   0   3   0   0   0  16   4   0   0\n",
      "    1  11   0  24   1   0  28   0   1]\n",
      " [  0   0   0   0   0   1   2   0  15   0   0   0   0   0   0   0   0   0\n",
      "    0  41   0   5   0   2   9   0   0]\n",
      " [  0   5   0   0   0   0   0   0  14   0  23   0   0   0   8  13   0   0\n",
      "    1  15   0  16   0   0 374   0   1]\n",
      " [  0   3   0   0   0   6   0   0  22   0   0   0   0   0   9  12   0   0\n",
      "    1   8   0   2   0   0  18   0   0]\n",
      " [  0   3   0   0   0   0   0   0  10   0   0   0   0   0   2 115   0   0\n",
      "    1   2   0  19   0   0  15   0  62]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andfe\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\andfe\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\andfe\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "for batch in val_dataloader:\n",
    "    b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    predictions.append(logits.argmax(dim=1).cpu().numpy())\n",
    "    true_labels.append(b_labels.cpu().numpy())\n",
    "\n",
    "predictions = np.concatenate(predictions)\n",
    "true_labels = np.concatenate(true_labels)\n",
    "\n",
    "print(\"Reporte de Clasificación:\\n\", classification_report(true_labels, predictions, target_names=label_encoder.classes_))\n",
    "print(\"Matriz de Confusión:\\n\", confusion_matrix(true_labels, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c65b8e",
   "metadata": {},
   "source": [
    "Para la evaluación final, se generan el reporte de clasificación, que incluye métricas como precisión, recall y F1-score, y la matriz de confusión para visualizar el rendimiento por categoría.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
